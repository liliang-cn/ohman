# LLM Configuration
llm:
  # Supported providers: openai, anthropic, ollama, custom
  provider: openai
  
  # API key (can also be set via OHMAN_API_KEY environment variable)
  api_key: ""
  
  # Custom API endpoint (optional, for proxies or self-hosted services)
  base_url: ""
  
  # Model name
  # OpenAI: gpt-4o, gpt-4o-mini, gpt-4-turbo, gpt-3.5-turbo
  # Anthropic: claude-3-5-sonnet-20241022, claude-3-haiku-20240307
  # Ollama: llama3, mistral, codellama, etc.
  model: gpt-4o-mini
  
  # Maximum output tokens
  max_tokens: 4096
  
  # Temperature parameter (0-2), lower is more deterministic, higher is more creative
  temperature: 0.7
  
  # Request timeout (seconds)
  timeout: 60

# Shell Configuration
shell:
  # Shell history file path (leave empty for auto-detection)
  # Zsh: ~/.zsh_history
  # Bash: ~/.bash_history
  history_file: ""
  
  # Enable auto-installation of failed command hook (on first run)
  auto_install_hook: true

# Output Configuration
output:
  # Enable colored output
  color: true
  
  # Enable Markdown rendering
  markdown: true
  
  # Output language (en-US, zh-CN)
  language: en-US

# Debug Configuration
debug:
  # Enable debug mode
  enabled: false
  
  # Show the full prompt sent to LLM
  show_prompt: false
  
  # Show token usage
  show_tokens: false
